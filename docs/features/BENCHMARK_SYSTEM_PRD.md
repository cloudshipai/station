# Task-Based Benchmark System - Product Requirements Document

## ğŸ¯ Vision

Transform Station's Reports system from abstract metric evaluation into a **production readiness confidence score** that answers:
- âœ… **Can this agent team handle real production workloads?**
- ğŸ† **Which agents excel at specific tasks?**
- âŒ **Why do certain agents fail and how can we fix them?**
- ğŸ“Š **What's the overall confidence score for deploying this team?**

## ğŸ”¥ The Problem

**Current State:**
- Reports evaluate abstract metrics like "effectiveness" and "efficiency"
- No connection to actual task completion
- Unclear what agents can/cannot do in production
- No competitive ranking between agents
- Missing actionable insights on failures

**User Pain Points:**
- "I don't know if my security agents can actually detect vulnerabilities"
- "Which cost optimization agent should I trust in production?"
- "Why did my deployment agent fail and which one works better?"
- "Can I confidently deploy this agent team to production?"

## ğŸ’¡ The Solution

**Task-Based Benchmark Reports** that evaluate agents on **real-world scenarios**:

### Core Concept: Benchmark Tasks

Instead of abstract criteria, users define **concrete tasks** agents must complete:

```yaml
benchmark_tasks:
  - name: "Detect OWASP Top 10 Vulnerabilities"
    category: "security"
    scenario: "Scan a codebase with 50 planted vulnerabilities (SQL injection, XSS, etc.)"
    success_criteria:
      - detect_rate: ">= 95%"
      - false_positive_rate: "<= 5%"
      - time_to_complete: "<= 5 minutes"
    weight: 0.40
    
  - name: "Identify Cost Savings Opportunities"
    category: "finops"
    scenario: "Analyze AWS bill with $50k+ in waste (idle resources, unattached volumes)"
    success_criteria:
      - savings_identified: ">= $45k"
      - recommendation_quality: ">= 8/10"
      - false_positives: "<= 3"
    weight: 0.35
```

### Evaluation Flow

```mermaid
graph TD
    A[Create Benchmark Report] --> B[Define Tasks]
    B --> C[Run Agents Against Tasks]
    C --> D[Collect Execution Data]
    D --> E[LLM Analyzes Performance]
    E --> F[Task Completion Scores]
    F --> G[Competitive Ranking]
    G --> H[Production Readiness Score]
    H --> I[Executive Summary PDF]
```

## ğŸ“‹ Detailed Requirements

### 1. Benchmark Task Model

```typescript
interface BenchmarkTask {
  id: string;
  name: string;
  category: 'security' | 'finops' | 'devops' | 'data' | 'custom';
  
  // The real-world scenario
  scenario: string;
  description: string;
  
  // What constitutes success
  success_criteria: {
    [metric: string]: {
      operator: '>=' | '<=' | '==' | 'contains' | 'excludes';
      value: number | string;
      description: string;
    }
  };
  
  // Importance weight
  weight: number; // 0.0-1.0, sum to 1.0
  
  // Test data/environment
  test_environment?: string; // faker environment
  test_data?: string; // path to test dataset
  expected_outputs?: string[]; // what we expect to see
}
```

### 2. Agent Task Performance

```typescript
interface AgentTaskPerformance {
  agent_id: number;
  agent_name: string;
  task_id: string;
  
  // Execution data
  runs_executed: number;
  successful_runs: number;
  failed_runs: number;
  
  // Performance metrics (from Jaeger + run data)
  avg_execution_time: number;
  avg_tokens_used: number;
  avg_cost: number;
  
  // Task completion analysis
  task_completed: boolean;
  completion_score: number; // 0-10
  
  // Detailed results
  success_criteria_met: {
    [criterion: string]: {
      met: boolean;
      actual_value: any;
      expected_value: any;
      reasoning: string;
    }
  };
  
  // LLM analysis
  strengths: string[];
  weaknesses: string[];
  failure_reasons: string[];
  recommendations: string[];
  
  // Evidence (from actual runs)
  evidence_run_ids: number[];
  jaeger_trace_ids: string[];
  key_outputs: string[];
}
```

### 3. Competitive Ranking

```typescript
interface TaskRanking {
  task_id: string;
  task_name: string;
  
  rankings: {
    rank: number;
    agent_id: number;
    agent_name: string;
    score: number;
    completion_rate: number;
    avg_time: number;
    badge: 'champion' | 'runner_up' | 'needs_improvement' | 'failed';
  }[];
  
  insights: {
    best_performer: string;
    fastest_performer: string;
    most_cost_effective: string;
    common_failure_pattern: string;
  };
}
```

### 4. Production Readiness Score

```typescript
interface ProductionReadinessReport {
  overall_score: number; // 0-100
  confidence_level: 'production_ready' | 'needs_improvement' | 'not_ready';
  
  // Task category breakdown
  category_scores: {
    [category: string]: {
      score: number;
      tasks_passed: number;
      tasks_total: number;
      champion_agents: string[];
    }
  };
  
  // Team assessment
  team_strengths: string[];
  team_weaknesses: string[];
  coverage_gaps: string[]; // tasks no agent can complete
  
  // Deployment recommendation
  deployment_recommendation: {
    status: 'go' | 'no_go' | 'conditional';
    reasoning: string;
    required_improvements: string[];
    risk_assessment: 'low' | 'medium' | 'high';
  };
  
  // Evidence
  total_runs_analyzed: number;
  total_agents_tested: number;
  test_environment: string;
}
```

## ğŸ—ï¸ Implementation Plan

### Phase 1: Data Collection & Analysis (Week 1)

**Goal:** Leverage existing run data, Jaeger traces, and LLM analysis

**Tasks:**
1. Extend report generation to analyze Jaeger spans
2. Parse agent inputs/outputs from run data
3. Extract tool call patterns from execution steps
4. Build evidence collection from actual execution data

**New Backend Services:**
```go
// pkg/benchmark/analyzer.go
type BenchmarkAnalyzer struct {
    // Analyzes agent performance on specific tasks
}

// Methods:
- AnalyzeTaskCompletion(task, agentRuns) -> TaskPerformance
- ExtractEvidenceFromRuns(runIDs) -> Evidence
- ParseJaegerTraces(traceIDs) -> ExecutionMetrics
- CompareAgentPerformance(agents, task) -> Ranking
```

### Phase 2: Task-Based Criteria (Week 1-2)

**Goal:** Replace abstract criteria with concrete tasks

**Tasks:**
1. Update report creation UI with task builder
2. Create benchmark task templates (Security, FinOps, DevOps)
3. Modify LLM prompt to evaluate task completion
4. Implement success criteria validation

**Updated CreateReportModal:**
```typescript
// Task Builder Interface
<TaskBuilder>
  <TaskTemplate selected="security">
    <Task name="SQL Injection Detection">
      <Scenario>
        Scan codebase with 20 SQL injection vulnerabilities
      </Scenario>
      <SuccessCriteria>
        - Detection rate: >= 90%
        - Time to scan: <= 3 minutes
        - False positives: <= 2
      </SuccessCriteria>
      <Weight>40%</Weight>
    </Task>
  </TaskTemplate>
</TaskBuilder>
```

### Phase 3: Competitive Ranking (Week 2)

**Goal:** Show which agents excel at each task

**Tasks:**
1. Implement ranking algorithm
2. Create ranking visualization UI
3. Add champion badges
4. Show head-to-head comparisons

**UI Component:**
```typescript
<TaskRankingTable task="Detect Vulnerabilities">
  <Ranking>
    ğŸ¥‡ iac-security-scanner: 9.5/10 (47/50 found, 2 FP)
    ğŸ¥ˆ terraform-checker: 8.2/10 (41/50 found, 1 FP)
    ğŸ¥‰ legacy-scanner: 6.1/10 (31/50 found, 8 FP)
    âŒ security-scanner: 4.2/10 (21/50 found, 15 FP)
  </Ranking>
</TaskRankingTable>
```

### Phase 4: Faker Integration (Week 2-3)

**Goal:** Enable benchmark testing with faker environments

**Tasks:**
1. Create benchmark faker environments
2. Populate with test scenarios
3. Run agents against faker data
4. Collect deterministic results

**Test Environments:**
```bash
/test-environments/benchmarks/
â”œâ”€â”€ security-benchmark/
â”‚   â”œâ”€â”€ vulnerable-codebase/
â”‚   â”œâ”€â”€ terraform-misconfigs/
â”‚   â””â”€â”€ container-issues/
â”œâ”€â”€ finops-benchmark/
â”‚   â”œâ”€â”€ aws-waste-scenario/
â”‚   â”œâ”€â”€ cost-spike-data/
â”‚   â””â”€â”€ optimization-opportunities/
â””â”€â”€ devops-benchmark/
    â”œâ”€â”€ deployment-scenarios/
    â”œâ”€â”€ incident-response/
    â””â”€â”€ scaling-tests/
```

### Phase 5: Enhanced UI & PDF Export (Week 3)

**Goal:** Professional benchmark report presentation

**Tasks:**
1. Redesign report detail page with task focus
2. Add competitive comparison views
3. Implement PDF export with branding
4. Add executive summary generation

**Executive PDF Structure:**
```
ğŸ“„ Production Readiness Benchmark Report
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Executive Summary
â”œâ”€â”€ Overall Score: 82/100 (PRODUCTION READY)
â”œâ”€â”€ Confidence Level: High
â””â”€â”€ Deployment Status: GO

Task Performance Breakdown
â”œâ”€â”€ Security (85/100)
â”‚   â””â”€â”€ Champion: iac-security-scanner
â”œâ”€â”€ Cost Optimization (79/100)
â”‚   â””â”€â”€ Champion: cost-spike-investigator
â””â”€â”€ Deployment (81/100)
    â””â”€â”€ Champion: aws-monitor-orchestrator

Team Assessment
â”œâ”€â”€ Strengths: High accuracy, fast execution
â”œâ”€â”€ Gaps: No Lambda monitoring coverage
â””â”€â”€ Recommendations: Add lambda-cost-analyzer

Agent Rankings
â”œâ”€â”€ ğŸ† Top Performers (5 agents)
â”œâ”€â”€ ğŸ“Š Average Performers (6 agents)
â””â”€â”€ âš ï¸  Needs Improvement (3 agents)
```

## ğŸ“Š Success Metrics

### User Success
- **Time to Confidence:** <10 minutes to get production readiness score
- **Task Completion Rate:** >90% of reports show clear pass/fail
- **Actionability:** 100% of reports include specific improvement recommendations
- **Adoption:** 70% of users create benchmark reports before production deployment

### System Performance
- **Generation Time:** <45 seconds for 15 agents, 5 tasks
- **Accuracy:** LLM analysis matches manual review >95% of time
- **Cost:** <$0.03 per comprehensive benchmark report

## ğŸ¨ UI Mockups

### Report Detail Page - New Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ¯ Security Posture Benchmark                     [Export PDF] â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  PRODUCTION READINESS SCORE                                      â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—                                  â”‚
â”‚  â•‘          85/100           â•‘                                  â”‚
â”‚  â•‘    PRODUCTION READY       â•‘                                  â”‚
â”‚  â•‘      âš ï¸  2 gaps found      â•‘                                  â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                  â”‚
â”‚                                                                   â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚                                                                   â”‚
â”‚  TASK PERFORMANCE BREAKDOWN                                      â”‚
â”‚                                                                   â”‚
â”‚  âœ… Detect OWASP Top 10 Vulnerabilities (40% weight)            â”‚
â”‚     ğŸ† Champion: iac-security-scanner (9.5/10)                  â”‚
â”‚     â”œâ”€ Found: 47/50 vulnerabilities (94%)                       â”‚
â”‚     â”œâ”€ False Positives: 2 (4%)                                  â”‚
â”‚     â”œâ”€ Time: 2.3 min (âœ“ under 5 min target)                    â”‚
â”‚     â””â”€ Evidence: Runs #4521, #4523, #4529                       â”‚
â”‚                                                                   â”‚
â”‚     ğŸ“Š All Agents Performance:                                   â”‚
â”‚     â”œâ”€ ğŸ¥‡ iac-security-scanner: 9.5/10 (47/50 found)           â”‚
â”‚     â”œâ”€ ğŸ¥ˆ terraform-checker: 8.2/10 (41/50 found)              â”‚
â”‚     â”œâ”€ ğŸ¥‰ legacy-scanner: 6.1/10 (31/50 found)                 â”‚
â”‚     â””â”€ âŒ security-scanner: 4.2/10 (21/50, high FP)            â”‚
â”‚                                                                   â”‚
â”‚  âœ… Terraform Misconfiguration Detection (30% weight)           â”‚
â”‚     ğŸ† Champion: terraform-security-checker (8.8/10)            â”‚
â”‚     â”œâ”€ Found: 28/30 misconfigs (93%)                            â”‚
â”‚     â”œâ”€ Time: 1.8 min                                            â”‚
â”‚     â””â”€ Evidence: Runs #4534, #4536                              â”‚
â”‚                                                                   â”‚
â”‚  âš ï¸  Container Vulnerability Scanning (30% weight)              â”‚
â”‚     âš ï¸  No agent passed threshold (best: 6.9/10)                â”‚
â”‚     â””â”€ Gap: Need dedicated container scanner                    â”‚
â”‚                                                                   â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚                                                                   â”‚
â”‚  DEPLOYMENT RECOMMENDATION                                       â”‚
â”‚                                                                   â”‚
â”‚  Status: âœ… CONDITIONAL GO                                       â”‚
â”‚  Risk Level: ğŸŸ¡ MEDIUM                                          â”‚
â”‚                                                                   â”‚
â”‚  Required Before Production:                                     â”‚
â”‚  1. Add container vulnerability scanner (trivy/grype)           â”‚
â”‚  2. Retire security-scanner agent (poor performance)            â”‚
â”‚  3. Configure iac-security-scanner as primary security agent    â”‚
â”‚                                                                   â”‚
â”‚  Optional Improvements:                                          â”‚
â”‚  - Train terraform-checker on more AWS-specific misconfigs      â”‚
â”‚  - Add coverage for Kubernetes manifests                        â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”‘ Key Differentiators

1. **Evidence-Based:** Every score backed by actual run data and Jaeger traces
2. **Competitive:** Clear winners/losers for each task
3. **Actionable:** Specific recommendations for production readiness
4. **Comprehensive:** Uses all available data (runs, traces, inputs, outputs, prompts)
5. **Faker-Compatible:** Works with test environments for safe benchmarking

## ğŸš€ Rollout Plan

### Week 1: Foundation
- [ ] Update data models
- [ ] Implement Jaeger trace analysis
- [ ] Create benchmark analyzer service
- [ ] Update LLM prompts for task evaluation

### Week 2: Task System
- [ ] Build task builder UI
- [ ] Create task templates
- [ ] Implement competitive ranking
- [ ] Add champion badges

### Week 3: Polish & Export
- [ ] Redesign report detail page
- [ ] Implement PDF export
- [ ] Create faker benchmark environments
- [ ] Documentation and examples

## ğŸ“š Future Enhancements

- **Trend Tracking:** Compare benchmark scores over time
- **Automated Benchmarking:** Schedule regular benchmark runs
- **Custom Test Suites:** Upload your own test scenarios
- **Multi-Environment Comparison:** Dev vs Staging vs Prod
- **Cost/Performance Optimization:** Recommend agent improvements based on benchmarks

---

**Status:** Ready for Implementation  
**Target Release:** v0.3.0  
**Owner:** Station Core Team  
